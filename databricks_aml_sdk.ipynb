{"cells":[{"cell_type":"markdown","source":["This notebook is based on the Azure ML & Azure Databricks notebooks by Parashar Shah\n(ref. https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/azure-databricks/amlsdk )\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"markdown","source":["We support installing AML SDK as library from GUI. When attaching a library follow this https://docs.databricks.com/user-guide/libraries.html and add the below string as your PyPi package. You can select the option to attach the library to all clusters or just one cluster.\n\n**install azureml-sdk**\n* Source: Upload Python Egg or PyPi\n* PyPi Name: `azureml-sdk`\n* Select Install Library"],"metadata":{}},{"cell_type":"code","source":["import azureml.core\n\n# Check core SDK version number - based on build number of preview/master.\nprint(\"SDK version:\", azureml.core.VERSION)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Please specify the Azure subscription Id, resource group name, workspace name, and the region in which you want to create the Azure Machine Learning Workspace.\n\nYou can get the value of your Azure subscription ID from the Azure Portal, and then selecting Subscriptions from the menu on the left.\n\nFor the resource_group, use the name of the resource group that contains your Azure Databricks Workspace.\n\nNOTE: If you provide a resource group name that does not exist, the resource group will be automatically created. This may or may not succeed in your environment, depending on the permissions you have on your Azure Subscription."],"metadata":{}},{"cell_type":"code","source":["# subscription_id = \"<your-subscription-id>\"\n# resource_group = \"<your-existing-resource-group>\"\n# workspace_name = \"<a-new-or-existing-workspace; it is unrelated to Databricks workspace>\"\n# workspace_region = \"<your-resource group-region>\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Set auth to be used by workspace related APIs.\n# For automation or CI/CD ServicePrincipalAuthentication can be used.\n# https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.authentication.serviceprincipalauthentication?view=azure-ml-py\nauth = None"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# import the Workspace class and check the azureml SDK version\n# exist_ok checks if workspace exists or not.\n\nfrom azureml.core import Workspace\n\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      auth = auth,\n                      exist_ok=True)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#get workspace details\nws.get_details()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["ws = Workspace(workspace_name = workspace_name,\n               subscription_id = subscription_id,\n               resource_group = resource_group,\n               auth = auth)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# find out what you can do with Azure ML class Workspace\nhelp(Workspace)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#02 Ingest Data"],"metadata":{}},{"cell_type":"markdown","source":["#Data Ingestion"],"metadata":{}},{"cell_type":"code","source":["import os\nimport urllib"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Download AdultCensusIncome.csv from Azure CDN. This file has 32,561 rows.\ndataurl = \"https://amldockerdatasets.azureedge.net/AdultCensusIncome.csv\"\ndatafile = \"AdultCensusIncome.csv\"\ndatafile_dbfs = os.path.join(\"/dbfs\", datafile)\n\nif os.path.isfile(datafile_dbfs):\n    print(\"found {} at {}\".format(datafile, datafile_dbfs))\nelse:\n    print(\"downloading {} to {}\".format(datafile, datafile_dbfs))\n    urllib.request.urlretrieve(dataurl, datafile_dbfs)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Create a Spark dataframe out of the csv file.\ndata_all = sqlContext.read.format('csv').options(header='true', inferSchema='true', ignoreLeadingWhiteSpace='true', ignoreTrailingWhiteSpace='true').load(datafile)\nprint(\"({}, {})\".format(data_all.count(), len(data_all.columns)))\ndata_all.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#renaming columns\ncolumns_new = [col.replace(\"-\", \"_\") for col in data_all.columns]\ndata_all = data_all.toDF(*columns_new)\ndata_all.printSchema()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(data_all.limit(5))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#Data Preparation"],"metadata":{}},{"cell_type":"code","source":["# Choose feature columns and the label column.\nlabel = \"income\"\nxvars = set(data_all.columns) - {label}\n\nprint(\"label = {}\".format(label))\nprint(\"features = {}\".format(xvars))\n\ndata = data_all.select([*xvars, label])\n\n# Split data into train and test.\ntrain, test = data.randomSplit([0.75, 0.25], seed=123)\n\nprint(\"train ({}, {})\".format(train.count(), len(train.columns)))\nprint(\"test ({}, {})\".format(test.count(), len(test.columns)))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#Data Persistence"],"metadata":{}},{"cell_type":"code","source":["# Write the train and test data sets to intermediate storage\ntrain_data_path = \"AdultCensusIncomeTrain\"\ntest_data_path = \"AdultCensusIncomeTest\"\n\ntrain_data_path_dbfs = os.path.join(\"/dbfs\", \"AdultCensusIncomeTrain\")\ntest_data_path_dbfs = os.path.join(\"/dbfs\", \"AdultCensusIncomeTest\")\n\ntrain.write.mode('overwrite').parquet(train_data_path)\ntest.write.mode('overwrite').parquet(test_data_path)\nprint(\"train and test datasets saved to {} and {}\".format(train_data_path_dbfs, test_data_path_dbfs))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#03 Model Building"],"metadata":{}},{"cell_type":"markdown","source":["#Model Building"],"metadata":{}},{"cell_type":"code","source":["import os\nimport pprint\nimport numpy as np\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#get the train and test datasets\ntrain_data_path = \"AdultCensusIncomeTrain\"\ntest_data_path = \"AdultCensusIncomeTest\"\n\ntrain = spark.read.parquet(train_data_path)\ntest = spark.read.parquet(test_data_path)\n\nprint(\"train: ({}, {})\".format(train.count(), len(train.columns)))\nprint(\"test: ({}, {})\".format(test.count(), len(test.columns)))\n\ntrain.printSchema()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["#Define Model"],"metadata":{}},{"cell_type":"code","source":["label = \"income\"\ndtypes = dict(train.dtypes)\ndtypes.pop(label)\n\nsi_xvars = []\nohe_xvars = []\nfeatureCols = []\nfor idx,key in enumerate(dtypes):\n    if dtypes[key] == \"string\":\n        featureCol = \"-\".join([key, \"encoded\"])\n        featureCols.append(featureCol)\n        \n        tmpCol = \"-\".join([key, \"tmp\"])\n        # string-index and one-hot encode the string column\n        #https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/feature/StringIndexer.html\n        #handleInvalid: Param for how to handle invalid data (unseen labels or NULL values). \n        #Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), \n        #or 'keep' (put invalid data in a special additional bucket, at index numLabels). Default: \"error\"\n        si_xvars.append(StringIndexer(inputCol=key, outputCol=tmpCol, handleInvalid=\"skip\"))\n        ohe_xvars.append(OneHotEncoder(inputCol=tmpCol, outputCol=featureCol))\n    else:\n        featureCols.append(key)\n\n# string-index the label column into a column named \"label\"\nsi_label = StringIndexer(inputCol=label, outputCol='label')\n\n# assemble the encoded feature columns in to a column named \"features\"\nassembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\nimport numpy as np\nimport os\nimport shutil\n\nmodel_name = \"AdultCensus_runHistory.mml\"\nmodel_dbfs = os.path.join(\"/dbfs\", model_name)\nrun_history_name = 'spark-ml-notebook'\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, \"Ignite_AI_Talk\")\nroot_run = myexperiment.start_logging()\n\n# Regularization Rates - \nregs = [0.0001, 0.001, 0.01, 0.1]\n \n# try a bunch of regularization rate in a Logistic Regression model\nfor reg in regs:\n    print(\"Regularization rate: {}\".format(reg))\n    # create a bunch of child runs\n    with root_run.child_run(\"reg-\" + str(reg)) as run:\n        # create a new Logistic Regression model.\n        lr = LogisticRegression(regParam=reg)\n        \n        # put together the pipeline\n        pipe = Pipeline(stages=[*si_xvars, *ohe_xvars, si_label, assembler, lr])\n\n        # train the model\n        model_p = pipe.fit(train)\n        \n        # make prediction\n        pred = model_p.transform(test)\n        \n        # evaluate. note only 2 metrics are supported out of the box by Spark ML.\n        bce = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n        au_roc = bce.setMetricName('areaUnderROC').evaluate(pred)\n        au_prc = bce.setMetricName('areaUnderPR').evaluate(pred)\n\n        print(\"Area under ROC: {}\".format(au_roc))\n        print(\"Area Under PR: {}\".format(au_prc))\n      \n        # log reg, au_roc, au_prc and feature names in run history\n        run.log(\"reg\", reg)\n        run.log(\"au_roc\", au_roc)\n        run.log(\"au_prc\", au_prc)\n        run.log_list(\"columns\", train.columns)\n\n        # save model\n        model_p.write().overwrite().save(model_name)\n        \n        # upload the serialized model into run history record\n        mdl, ext = model_name.split(\".\")\n        model_zip = mdl + \".zip\"\n        shutil.make_archive(mdl, 'zip', model_dbfs)\n        run.upload_file(\"outputs/\" + model_name, model_zip)        \n        #run.upload_file(\"outputs/\" + model_name, path_or_stream = model_dbfs) #cannot deal with folders\n\n        # now delete the serialized model from local folder since it is already uploaded to run history \n        shutil.rmtree(model_dbfs)\n        os.remove(model_zip)\n        \n# Declare run completed\nroot_run.complete()\nroot_run_id = root_run.id\nprint (\"run id:\", root_run.id)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["metrics = root_run.get_metrics(recursive=True)\nbest_run_id = max(metrics, key = lambda k: metrics[k]['au_roc'])\nprint(best_run_id, metrics[best_run_id]['au_roc'], metrics[best_run_id]['reg'])"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#Get the best run\nchild_runs = {}\n\nfor r in root_run.get_children():\n    child_runs[r.id] = r\n   \nbest_run = child_runs[best_run_id]"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#Download the model from the best run to a local folder\nbest_model_file_name = \"best_model.zip\"\nbest_run.download_file(name = 'outputs/' + model_name, output_file_path = best_model_file_name)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["#Model Evaluation"],"metadata":{}},{"cell_type":"code","source":["##unzip the model to dbfs (as load() seems to require that) and load it.\nif os.path.isfile(model_dbfs) or os.path.isdir(model_dbfs):\n    shutil.rmtree(model_dbfs)\nshutil.unpack_archive(best_model_file_name, model_dbfs)\n\nmodel_p_best = PipelineModel.load(model_name)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# make prediction\npred = model_p_best.transform(test)\noutput = pred[['hours_per_week','age','workclass','marital_status','income','prediction']]\ndisplay(output.limit(5))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# evaluate. note only 2 metrics are supported out of the box by Spark ML.\nbce = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\nau_roc = bce.setMetricName('areaUnderROC').evaluate(pred)\nau_prc = bce.setMetricName('areaUnderPR').evaluate(pred)\n\nprint(\"Area under ROC: {}\".format(au_roc))\nprint(\"Area Under PR: {}\".format(au_prc))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["#Model Persistence"],"metadata":{}},{"cell_type":"code","source":["##NOTE: by default the model is saved to and loaded from /dbfs/ instead of cwd!\nmodel_p_best.write().overwrite().save(model_name)\nprint(\"saved model to {}\".format(model_dbfs))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%sh\n\n# check the model has been saved\n\nls -la /dbfs/AdultCensus_runHistory.mml/*"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["# 04 Deploy Model"],"metadata":{}},{"cell_type":"markdown","source":["# Register Azure Databricks trained model and deploy it to ACI"],"metadata":{}},{"cell_type":"markdown","source":["Please ensure you have run all previous notebooks in sequence before running this.\n\nPlease Register Azure Container Instance(ACI) using Azure Portal: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-supported-services#portal in your subscription before using the SDK to deploy your ML model to ACI."],"metadata":{}},{"cell_type":"code","source":["##NOTE: service deployment always gets the model from the current working dir.\nimport os\n\nmodel_name = \"AdultCensus_runHistory.mml\" # \nmodel_name_dbfs = os.path.join(\"/dbfs\", model_name)\n\nprint(\"copy model from dbfs to local\")\nmodel_local = \"file:\" + os.getcwd() + \"/\" + model_name\ndbutils.fs.cp(model_name, model_local, True)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["#Register the model\nfrom azureml.core.model import Model\nmymodel = Model.register(model_path = model_name, # this points to a local file\n                       model_name = model_name, # this is the name the model is registered as, am using same name for both path and name.                 \n                       description = \"ADB trained model by Parashar\",\n                       workspace = ws)\n\nprint(mymodel.name, mymodel.description, mymodel.version)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["#%%writefile score_sparkml.py\nscore_sparkml = \"\"\"\n \nimport json\n \ndef init():\n    # One-time initialization of PySpark and predictive model\n    import pyspark\n    import os\n    from azureml.core.model import Model\n    from pyspark.ml import PipelineModel\n \n    global trainedModel\n    global spark\n \n    spark = pyspark.sql.SparkSession.builder.appName(\"ADB and AML notebook by Parashar\").getOrCreate()\n    model_name = \"{model_name}\" #interpolated\n    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), model_name)\n    trainedModel = PipelineModel.load(model_path)\n    \ndef run(input_json):\n    if isinstance(trainedModel, Exception):\n        return json.dumps({{\"trainedModel\":str(trainedModel)}})\n      \n    try:\n        sc = spark.sparkContext\n        input_list = json.loads(input_json)\n        input_rdd = sc.parallelize(input_list)\n        input_df = spark.read.json(input_rdd)\n    \n        # Compute prediction\n        prediction = trainedModel.transform(input_df)\n        #result = prediction.first().prediction\n        predictions = prediction.collect()\n \n        #Get each scored result\n        preds = [str(x['prediction']) for x in predictions]\n        result = \",\".join(preds)\n        # you can return any data type as long as it is JSON-serializable\n        # return result.tolist()\n        return result\n    except Exception as e:\n        result = str(e)\n        return result\n    \n\"\"\".format(model_name=model_name)\n \nexec(score_sparkml)\n \nwith open(\"score_sparkml.py\", \"w\") as file:\n    file.write(score_sparkml)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["#deploy to ACI\nfrom azureml.core.webservice import AciWebservice, Webservice\nfrom azureml.exceptions import WebserviceException\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\nmyaci_config = AciWebservice.deploy_configuration(cpu_cores = 2, \n                                                  memory_gb = 2, \n                                                  tags = {'name':'Databricks Azure ML ACI'}, \n                                                  description = 'This is for ADB and AML example.')\n\nservice_name = 'aciws'\n\n# Remove any existing service under the same name.\ntry:\n    Webservice(ws, service_name).delete()\nexcept WebserviceException:\n    pass\n\nmyenv = Environment.get(ws, name='AzureML-PySpark-MmlSpark-0.15')\n# we need to add extra packages to procured environment\n# in order to deploy amended environment we need to rename it\nmyenv.name = 'myenv'\ninference_config = InferenceConfig(entry_script='score_sparkml.py', environment=myenv)\n\nmyservice = Model.deploy(ws, service_name, [mymodel], inference_config, myaci_config)\nmyservice.wait_for_deployment(show_output=True)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# learn about Webservice\nhelp(Webservice)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["#for using the Web HTTP API \nprint(myservice.scoring_uri)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["import json\n\n#get the some sample data\ntest_data_path = \"AdultCensusIncomeTest\"\ntest = spark.read.parquet(test_data_path).limit(5)\n\ntest_json = json.dumps(test.toJSON().collect())\n\nprint(test_json)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["#using data defined above predict if income is >50K (1) or <=50K (0)\nmyservice.run(input_data=test_json)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["#comment to not delete the web service\n# myservice.delete()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## Deploying to other types of computes\n\nIn order to learn how to deploy to other types of compute targets, such as AKS, please take a look at the set of notebooks in the [deployment](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment) folder."],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.6","nbconvert_exporter":"python","file_extension":".py"},"name":"installation-and-configuration-01","notebookId":2606284367497027,"kernelspec":{"display_name":"Python 3.6","language":"python","name":"python36"},"authors":[{"name":"pasha"}]},"nbformat":4,"nbformat_minor":0}
